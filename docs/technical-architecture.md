# Technical Architecture

This document provides a detailed overview of the technical architecture for the Threat Intelligence project. It covers the main components, data flow, technologies used, and deployment strategy.

## 1. Overview

The system is designed to ingest threat intelligence data (CVE and MITRE ATT&CK), process it into a structured knowledge graph format, embed this data into vector databases, and provide an API for querying the information using natural language.

The architecture follows a modular approach, separating concerns into distinct layers: API, Processing, Data Modeling, Database Embedding, and Inference.

## 2. Components

### 2.1. API Layer (`app/app.py`)

*   **Framework:** FastAPI
*   **Purpose:** Provides the external interface for interacting with the system.
*   **Endpoints:**
    *   `/upload` (POST): Accepts file uploads (JSON) along with `data_type` (CVE/MITRE) and `platform`. Saves the uploaded file and triggers the processing pipeline by executing `main.py` as a subprocess.
    *   `/embed` (POST): Accepts `data_type` and `platform`. Triggers the embedding process for a specified knowledge graph directory by executing `main.py --embed` as a subprocess.
    *   `/query` (GET): Accepts a natural language `query` string. Executes the `inference/query_databases.py` script as a subprocess to perform similarity search and generate a response.
*   **Request/Response Handling:** Uses Pydantic models (`app/schemas/`) for request validation and response formatting (`QueryRequestSchema`, `QueryResponseSchema`, `ErrorResponseSchema`, plus internal models in `app.py`).
*   **Configuration:** Reads basic settings from `app/config.py` (e.g., `UPLOAD_FOLDER`).
*   **Deployment:** Served using Uvicorn.

### 2.2. Processing Orchestration (`app/main.py`)

*   **Purpose:** Acts as the main command-line interface (CLI) and orchestrator for the data processing and embedding pipeline. It is invoked by the API layer or can be run directly.
*   **Functionality:**
    *   Parses command-line arguments (`argparse`) to determine the operation (process file, embed KG).
    *   Calls the appropriate processing function (`process_data`) based on `data_type`.
    *   Calls the knowledge graph generation function (`generate_knowledge_graph`, `process_mitre_data`).
    *   Handles the `--embed` flag by invoking the database update script (`ingestion/db_updater.py`).
*   **Interaction:** Calls specific processor scripts (`ingestion/cve_processor.py`, `ingestion/mitre_processor.py`) and KG generators (`ingestion/mitre_kg_generator.py`).

### 2.3. Data Ingestion & Processing (`app/ingestion/`)

*   **`cve_processor.py`:**
    *   **Input:** CVE JSON 5.0 format file path.
    *   **Processing:** Parses the JSON, validates structure, extracts nodes (CVE, Product, Vendor, CPE, CWE, Reference, Event) and relationships based on `models/cve.py`.
    *   **Output:** Returns a list of dictionaries representing the extracted graph elements. This list is directly saved as a CSV file by `main.py`.
*   **`mitre_processor.py`:**
    *   **Input:** MITRE STIX 2.x JSON file path and target `platform`.
    *   **Processing:** Filters `attack-pattern` objects based on the specified platform (using `PLATFORM_MAPPING`). Extracts each relevant technique and its directly related objects into separate JSON files.
    *   **Output:** Prints a JSON list containing the paths to the generated individual technique JSON files to stdout.
*   **`mitre_kg_generator.py`:**
    *   **Input:** Path to an individual MITRE technique JSON file (generated by `mitre_processor.py`).
    *   **Processing:** Parses the technique JSON, extracts nodes (AttackPattern, Reference, Platform, Detection) and relationships based on `models/mitre.py`.
    *   **Output:** Returns a list of dictionaries representing the extracted graph elements for that specific technique. This list is saved as a CSV file by `main.py`.
*   **`knowledge_graph_generator.py`:** (Seems like an older/alternative implementation, potentially used for direct visualization or different KG format). Creates NetworkX graphs, saves GML/CSV, and uses `visualization_utils.py`. *Note: The primary pipeline in `main.py` uses `cve_processor` and `mitre_kg_generator` for CSV output.*
*   **`visualization_utils.py`:** Provides functions to save NetworkX graphs to GML and CSV, and generate visualizations using Matplotlib (PNG) and Cytoscape.js (HTML).
*   **`base_processor.py`:** Defines an abstract base class for processors (currently only implemented by `CVEProcessor`).
*   **`processor_factory.py`:** A factory pattern implementation (currently unused in `main.py`) to potentially create different processor instances.

### 2.4. Data Modeling (`app/models/`)

*   **Purpose:** Defines the structure of the knowledge graph elements (nodes and relationships) using Python dataclasses.
*   **Modules:**
    *   `cve.py`: Defines dataclasses for CVE-related entities (`CVENode`, `ProductNode`, `VendorNode`, `CPENode`, `CWENode`, `ReferenceNode`, `EventNode`) and their relationships (`AffectsRelationship`, `BelongsToRelationship`, `HasCPERelationship`, etc.). Includes `to_dict()` methods for serialization to dictionary format, suitable for CSV conversion.
    *   `mitre.py`: Defines dataclasses for MITRE ATT&CK related entities (`AttackPatternNode`, `ReferenceNode`, `PlatformNode`, `DetectionNode`) and their relationships (`HasPlatformRelationship`, `HasReferenceRelationship`, etc.). Includes `to_dict()` methods.
    *   `base.py`: Contains base dataclasses (unused).

### 2.5. Database Embedding (`app/ingestion/db_updater*.py`, `llm`)

*   **Technology:** Uses the `llm` CLI tool and its plugins (`llm-embed-jina`, `llm-gemini`) for generating vector embeddings and storing them.
*   **Storage:** SQLite (`db/cve.db`, `db/mitre.db`). Embeddings are stored in collections named after the target `platform` (e.g., `containers`, `windows`).
*   **Scripts:**
    *   `db_updater.py`: Embeds multiple CSV files from a specified directory using `llm embed-multi`. Checks for existing database/collection to avoid redundant embedding. Triggered by `main.py --embed`.
    *   `db_updater_row.py`: Embeds a single CSV file row-by-row using `llm embed`. Includes content length checks, retry logic, and timeout handling. Useful for large files or more resilient embedding. (Not currently integrated into the main API/CLI flow).
*   **Configuration:** Embedding model (`EMBEDDING_MODEL`) is configurable via environment variables (defaults to `jina-embeddings-v2-small-en`).

### 2.6. Inference/Querying (`app/inference/query_databases.py`, `llm`)

*   **Purpose:** Handles natural language queries against the embedded knowledge graph data.
*   **Technology:** Leverages the `llm` CLI tool for both subject extraction and final response generation, using a specified generative model (e.g., `gemini-2.0-flash-exp`).
*   **Process (`query_databases.py`):**
    1.  Takes a user `query` as input.
    2.  Calls `llm` to extract the main subject(s) from the query.
    3.  Reads database/collection pairs from the `DATABASES` environment variable (e.g., `db/cve.db:containers,db/mitre.db:containers`).
    4.  For each database/collection pair, executes `llm similar` using the extracted subject to find relevant embedded entries.
    5.  Concatenates the results from all `llm similar` calls.
    6.  Pipes the concatenated similarity results as context to a final `llm` call along with the original user query to generate a synthesized natural language answer.
    7.  Prints the final answer to stdout.
*   **Configuration:** LLM model (`LLM_MODEL`), number of similarity results (`NUM_RESULTS`), and database/collection list (`DATABASES`) are configured via environment variables.

### 2.7. Configuration (`app/config.py`, `.env`, Environment Variables)

*   **`app/config.py`:** Defines static configuration like upload directories and allowed file extensions.
*   **`.env` File:** Used via `python-dotenv` to load environment variables, especially for local development (e.g., API keys, model names).
*   **Environment Variables:** Heavily used for configuring runtime behavior, especially within Docker:
    *   `LLM_GEMINI_KEY`: API key for Gemini.
    *   `LLM_MODEL`: Generative model for inference.
    *   `EMBEDDING_MODEL`: Model for generating embeddings.
    *   `NUM_RESULTS`: Number of results for similarity search.
    *   `DATABASES`: List of database files and collection names for querying.
    *   `FLASK_APP`, `FLASK_DEBUG` (Note: These seem like leftovers, as the app uses FastAPI/Uvicorn).

### 2.8. Containerization (`Dockerfile`, `docker-compose.yml`)

*   **`Dockerfile`:**
    *   Based on `python:3.11-slim-buster`.
    *   Copies application code.
    *   Installs Python dependencies from `requirements.txt`.
    *   Installs necessary `llm` plugins (`llm-gemini`, `llm-embed-jina`) using `llm install`. Allows overriding via build args `LLM_MODEL_INSTALL` and `LLM_EMBED_INSTALL`.
    *   Creates necessary directories (`/app/db`, `/app/uploads`).
    *   Sets the default command to run the FastAPI app using Uvicorn.
*   **`docker-compose.yml`:**
    *   Defines the `api` service.
    *   Builds the image using the `Dockerfile`.
    *   Maps port 8000.
    *   Mounts volumes for application code (`../app:/app`) and persistent data (`../data:/data`, although `db` and `uploads` are inside `/app` in the Dockerfile - potential inconsistency?). *Correction: `UPLOAD_FOLDER` and `DB_DIR` in `config.py` point within `/app/data`, which is likely intended to map to the host's `./data` via the volume mount.*
    *   Passes environment variables from the host or a `.env` file to the container (API keys, model names, database config).

### 2.9. Utility Scripts (`app/scripts/`)

*   **`process_directory.sh`:** A shell script to iterate over JSON files in a directory and process each one using `main.py`, logging the output.

## 3. Data Flow

### 3.1. Upload and Processing Flow (CVE Example)

1.  **API Request:** User sends POST to `/upload` with `data_type=CVE`, `platform=containers`, and a CVE JSON file.
2.  **API Handling (`app.py`):**
    *   Validates request and file type.
    *   Saves the file to `app/data/uploads/`.
    *   Executes `python main.py CVE containers app/data/uploads/<filename>.json -v`.
3.  **Orchestration (`main.py`):**
    *   Calls `process_data(..., data_type='CVE', ...)`.
4.  **CVE Processing (`process_data` -> `CVEProcessor`):**
    *   `CVEProcessor.process()` reads the JSON file.
    *   Extracts nodes and relationships into dataclass instances (`models/cve.py`).
    *   Converts instances to dictionaries using `to_dict()`.
    *   Returns a list of dictionaries.
5.  **CSV Generation (`main.py`):**
    *   Takes the list of dictionaries from `process_data`.
    *   Writes the data to `knowledge_graphs/<filename>.csv`.
6.  **API Response:** `app.py` receives the exit code from `main.py`. If successful, returns a success message; otherwise, returns an error.

### 3.2. Upload and Processing Flow (MITRE Example)

1.  **API Request:** User sends POST to `/upload` with `data_type=MITRE`, `platform=containers`, and `enterprise-attack.json`.
2.  **API Handling (`app.py`):**
    *   Saves the file to `app/data/uploads/`.
    *   Executes `python main.py MITRE containers app/data/uploads/enterprise-attack.json -v`.
3.  **Orchestration (`main.py`):**
    *   Calls `process_data(..., data_type='MITRE', ...)`.
4.  **MITRE Filtering (`process_data` -> `mitre_processor.py`):**
    *   `mitre_processor.process_mitre()` reads the STIX JSON.
    *   Filters techniques based on `platform`.
    *   Creates individual JSON files for each matching technique in the `output/` directory.
    *   Prints a JSON list of the created file paths to stdout.
5.  **KG Generation (`main.py` -> `process_mitre_data` -> `generate_knowledge_graph` -> `MITREKGGenerator`):**
    *   `main.py` captures the list of file paths from `process_data` stdout.
    *   Iterates through each technique JSON file path (`process_mitre_data`).
    *   For each file, calls `generate_knowledge_graph(..., data_type='MITRE', ...)`.
    *   `generate_knowledge_graph` calls `MITREKGGenerator.process()`.
    *   `MITREKGGenerator` reads the technique JSON, extracts nodes/relationships (`models/mitre.py`), converts to dictionaries.
    *   `generate_knowledge_graph` saves the dictionaries to `knowledge_graphs/<technique_id>.csv`.
6.  **API Response:** `app.py` returns success/error based on `main.py`'s exit code.

### 3.3. Embedding Flow

1.  **API Request:** User sends POST to `/embed` with `{"data_type": "CVE", "platform": "containers"}`.
2.  **API Handling (`app.py`):**
    *   Executes `python main.py --embed CVE containers --kg-directory knowledge_graphs -v`.
3.  **Orchestration (`main.py`):**
    *   Detects `--embed` flag.
    *   Executes `python ingestion/db_updater.py CVE containers knowledge_graphs`.
4.  **Database Update (`db_updater.py`):**
    *   Determines database path (`db/cve.db`).
    *   Checks if database and collection (`containers`) exist.
    *   If collection doesn't exist, executes `llm embed-multi containers -d db/cve.db --files knowledge_graphs **/*.csv --store -m <embedding_model>`.
    *   `llm` reads CSV files, generates embeddings, and stores them in the specified SQLite database and collection.
5.  **API Response:** `app.py` returns success/error based on `main.py`'s exit code.

### 3.4. Query Flow

1.  **API Request:** User sends GET to `/query?query=What+vulnerabilities+affect+nginx?`.
2.  **API Handling (`app.py`):**
    *   Executes `python inference/query_databases.py "What vulnerabilities affect nginx?"`.
3.  **Inference (`query_databases.py`):**
    *   Calls `llm -m <llm_model> "Extract subject..."` to get the subject (e.g., "nginx vulnerabilities").
    *   Reads `DATABASES` env var (e.g., `db/cve.db:containers,db/mitre.db:containers`).
    *   For `db/cve.db:containers`, executes `llm similar containers -d db/cve.db -c "nginx vulnerabilities" -n <num_results>`.
    *   For `db/mitre.db:containers`, executes `llm similar containers -d db/mitre.db -c "nginx vulnerabilities" -n <num_results>`.
    *   Concatenates the stdout from both `llm similar` calls.
    *   Executes `llm -m <llm_model> "What vulnerabilities affect nginx?"` piping the concatenated results as context via stdin.
    *   Prints the final generated response from `llm` to stdout.
4.  **API Response:** `app.py` captures the stdout from `query_databases.py` and returns it as `{"result": "..."}`.

## 4. Key Technologies

*   **Backend Framework:** FastAPI
*   **API Server:** Uvicorn
*   **Data Validation:** Pydantic
*   **CLI:** `argparse`
*   **Data Processing:** Standard Python libraries (json, os, csv), Dataclasses
*   **Knowledge Graph (Internal):** NetworkX (primarily for visualization/alternative KG format)
*   **LLM Interaction & Embedding:** `llm` CLI tool and plugins (e.g., `llm-gemini`, `llm-embed-jina`)
*   **Vector Database:** SQLite (via `llm` and `sqlite-utils`)
*   **Containerization:** Docker, Docker Compose
*   **Visualization:** Matplotlib, Cytoscape.js
*   **Environment Management:** `python-dotenv`

## 5. Directory Structure

```
.
├── app/
│   ├── __init__.py
│   ├── app.py              # FastAPI application
│   ├── config.py           # Static configuration
│   ├── main.py             # Main CLI orchestrator
│   ├── requirements.txt    # Python dependencies
│   ├── data/               # Data directory (mounted volume)
│   │   ├── db/             # SQLite databases
│   │   └── uploads/        # Uploaded raw files
│   ├── ingestion/          # Data processing modules
│   │   ├── __init__.py
│   │   ├── base_processor.py
│   │   ├── cve_processor.py
│   │   ├── db_updater.py
│   │   ├── db_updater_row.py
│   │   ├── knowledge_graph_generator.py # (Alternative KG/Viz)
│   │   ├── mitre_kg_generator.py
│   │   ├── mitre_processor.py
│   │   ├── processor_factory.py
│   │   └── visualization_utils.py
│   ├── inference/          # Querying modules
│   │   ├── __init__.py
│   │   └── query_databases.py
│   ├── models/             # Data models (dataclasses)
│   │   ├── __init__.py
│   │   ├── base.py
│   │   ├── cve.py
│   │   └── mitre.py
│   ├── schemas/            # API schemas (Pydantic)
│   │   └── __init__.py
│   └── scripts/            # Utility scripts
│       └── process_directory.sh
├── docker/
│   ├── Dockerfile
│   └── docker-compose.yml
├── docs/
│   ├── curl-cmds.md
│   ├── project-capabilities.md
│   └── technical-architecture.md # (This file)
├── examples/               # Example input data
│   ├── cve/
│   └── mitre/
├── knowledge_graphs/       # Generated CSV/GML files
│   └── visualization/      # Generated PNG/HTML visualizations
│       └── cytoscape/
├── .env.example            # Example environment variables
├── .gitignore
├── LICENSE
└── README.md
```
*(Note: The `knowledge_graphs` directory is generated during processing, not part of the source structure)*

## 6. Deployment

The application is designed to be deployed using Docker and Docker Compose. The `docker-compose.yml` file defines the API service, manages dependencies, and handles environment variable configuration. Persistent data (databases, uploads, generated KGs) should ideally be managed using Docker volumes mapped to the appropriate directories (`/app/data`, `/app/knowledge_graphs`).

## 7. Potential Improvements / Considerations

*   **Error Handling:** Enhance error handling and reporting in subprocess calls within `app.py` and `main.py`.
*   **Asynchronous Processing:** For long-running processing/embedding tasks initiated via the API, consider using background task queues (e.g., Celery, RQ) instead of direct subprocess calls to avoid blocking API responses.
*   **KG Generation Consistency:** Clarify the role of `knowledge_graph_generator.py` versus the CSV generation within `main.py` using `cve_processor` and `mitre_kg_generator`. Consolidate if possible.
*   **Database Choice:** For larger scale, consider more robust vector database solutions (e.g., PostgreSQL with pgvector, dedicated vector DBs like Weaviate, Pinecone) instead of SQLite.
*   **Configuration Management:** Centralize configuration access, potentially using a dedicated settings management library. Clarify volume mapping in Docker Compose vs. paths in `config.py`.
*   **Testing:** Implement unit and integration tests for processors, models, and API endpoints.
*   **Security:** Review file upload handling, subprocess execution, and API security practices.
*   **`llm` Dependency:** The heavy reliance on the `llm` CLI tool makes the system tightly coupled to its functionality and availability. Consider using underlying libraries directly for more control if needed.
